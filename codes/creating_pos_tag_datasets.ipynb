{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the datasets\n",
    "import warnings\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.data import load\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(dataset, file_path, file_name):\n",
    "    \"\"\"Function to save the dataset in csv format.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset: pandas dataframe\n",
    "        The dataset to be saved in the csv format.\n",
    "    file_path: String\n",
    "        The path where the dataset is to be stored.\n",
    "    file_name: String\n",
    "        The name of the saved file.\n",
    "    \"\"\"\n",
    "    complete_file_path_with_name = file_path + file_name\n",
    "    dataset.to_csv(complete_file_path_with_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokens(sentence):\n",
    "    '''Function to create tokens from the sentences using\n",
    "    regexp_tokenize that only extracts alphanumeric characters.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sentence: string\n",
    "        The sentence which is to be tokenized.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tokens: list\n",
    "        List of all the tokens in the sentence.\n",
    "    '''\n",
    "    tokens = regexp_tokenize(sentence, pattern=r'\\w+')\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tag_dict(tokens):\n",
    "    '''Function to create a dictionary with the tag as key\n",
    "    and count of tag in sentence as value.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tokens: list\n",
    "        Contains the tokens whose tag and count is to be formed.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tag_list: list\n",
    "        List of tags in the sentence\n",
    "    tag_count: list\n",
    "        List containing the value of corresponding element\n",
    "        in tag_list\n",
    "    '''\n",
    "    tag_tuple = nltk.pos_tag(tokens)\n",
    "    tags = [a[1] for a in tag_tuple]\n",
    "    tag_set = list(set(tags))\n",
    "    # Creating tag_dict where key is the tag and value is the count\n",
    "    tag_dict = {ele: tags.count(ele) for ele in tag_set}\n",
    "    return list(tag_dict.keys()), list(tag_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pos_tag(dataset, taglist):\n",
    "    '''Function to create a dataset with columns as pos tag.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset: pandas dataframe\n",
    "        The dataset whose sentences are to be converted\n",
    "        into pos tags.\n",
    "    taglist: list\n",
    "        List containing the available tag names.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pos_dataset: pandas dataset\n",
    "        Dataset with the columns as the taglist and the count of\n",
    "        the sentences for each row of dataset.\n",
    "    '''\n",
    "    pos_dataset = pd.DataFrame(columns=taglist)\n",
    "    \n",
    "    # Accessing each row in the dataset\n",
    "    for _, row in dataset.iterrows():\n",
    "        # Tokenize the sentence of each row\n",
    "        tokens = create_tokens(row['news'])\n",
    "        # Create tag_list and tag_count of the sentence\n",
    "        tag_list, tag_count = create_tag_dict(tokens)\n",
    "        df1 = pd.DataFrame([tag_count], columns=tag_list)\n",
    "        pos_dataset = pos_dataset.append(df1, sort=True)\n",
    "        \n",
    "    # Resetting the index of the pos_dataset\n",
    "    pos_dataset = pos_dataset.reset_index(drop=True)\n",
    "    # Adding the label of the dataset to pos_dataset\n",
    "    pos_dataset['label'] = dataset['label']\n",
    "    # Filling NaNs with 0\n",
    "    pos_dataset.fillna(0, inplace=True)\n",
    "    \n",
    "    return pos_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "train_data = pd.read_csv('../datasets/train.csv')\n",
    "valid_data = pd.read_csv('../datasets/valid.csv')\n",
    "test_data = pd.read_csv('../datasets/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>True</td>\n",
       "      <td>Says an array of statistics show that conditio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>True</td>\n",
       "      <td>Two-thirds of the people who start out in mini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4388</th>\n",
       "      <td>True</td>\n",
       "      <td>Texans spend $2.5 billion gambling in our neig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2797</th>\n",
       "      <td>False</td>\n",
       "      <td>Says Mike Dovilla supports a plan that could a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>False</td>\n",
       "      <td>Says Hillary Clinton wants to have open borders.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               news\n",
       "820    True  Says an array of statistics show that conditio...\n",
       "1849   True  Two-thirds of the people who start out in mini...\n",
       "4388   True  Texans spend $2.5 billion gambling in our neig...\n",
       "2797  False  Says Mike Dovilla supports a plan that could a...\n",
       "1254  False   Says Hillary Clinton wants to have open borders."
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>True</td>\n",
       "      <td>Marcy Kaptur voted against a ban which would h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>True</td>\n",
       "      <td>Over the last 10 years, incomes for the top 1 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>True</td>\n",
       "      <td>Energy nominee Steven Chu has called coal \"his...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>False</td>\n",
       "      <td>Says that when Democrats controlled Congress a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>False</td>\n",
       "      <td>Says Barack Obama is a Muslim.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               news\n",
       "235    True  Marcy Kaptur voted against a ban which would h...\n",
       "709    True  Over the last 10 years, incomes for the top 1 ...\n",
       "553    True  Energy nominee Steven Chu has called coal \"his...\n",
       "65    False  Says that when Democrats controlled Congress a...\n",
       "1157  False                     Says Barack Obama is a Muslim."
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>True</td>\n",
       "      <td>Congress used earmarks for more than 200 years.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>False</td>\n",
       "      <td>The U.S. Supreme Court struck down Wisconsins ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>True</td>\n",
       "      <td>Florida is enjoying its lowest crime rate in 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>True</td>\n",
       "      <td>While 9,000 state employees were added to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>True</td>\n",
       "      <td>To this day, (the Cuban government) is a regim...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               news\n",
       "754   True    Congress used earmarks for more than 200 years.\n",
       "639  False  The U.S. Supreme Court struck down Wisconsins ...\n",
       "917   True  Florida is enjoying its lowest crime rate in 3...\n",
       "444   True  While 9,000 state employees were added to the ...\n",
       "799   True  To this day, (the Cuban government) is a regim..."
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets_json to\n",
      "[nltk_data]     C:\\Users\\Selvaram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets_json is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('tagsets_json')\n",
    "from nltk.help import upenn_tagset\n",
    "upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating POS tag dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets_json to\n",
      "[nltk_data]     C:\\Users\\Selvaram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets_json is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('tagsets_json')\n",
    "from nltk.help import upenn_tagset\n",
    "upenn_tagset()  # This prints the tag descriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets_json to\n",
      "[nltk_data]     C:\\Users\\Selvaram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets_json is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('tagsets_json')\n",
    "import json\n",
    "from nltk.data import find\n",
    "\n",
    "with open(find('help/tagsets_json/PY3_json/upenn_tagset.json')) as f:\n",
    "    tagdict = json.load(f)\n",
    "\n",
    "taglist = list(tagdict.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Selvaram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Selvaram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')  # If you use word_tokenize elsewhere\n",
    "\n",
    "# Tokenizer function\n",
    "def create_tokens(sentence):\n",
    "    return regexp_tokenize(sentence, pattern=r'\\w+')\n",
    "\n",
    "# POS tag dictionary builder\n",
    "def create_tag_dict(tokens):\n",
    "    tag_tuple = nltk.pos_tag(tokens)\n",
    "    tags = [tag for _, tag in tag_tuple]\n",
    "    tag_set = list(set(tags))\n",
    "    tag_count = {tag: tags.count(tag) for tag in tag_set}\n",
    "    return tag_set, tag_count\n",
    "\n",
    "# POS tagging for entire dataset\n",
    "def create_pos_tag(dataset, taglist):\n",
    "    pos_dataset = pd.DataFrame(columns=taglist)\n",
    "    for _, row in dataset.iterrows():\n",
    "        tokens = create_tokens(row['news'])\n",
    "        tag_list, tag_count = create_tag_dict(tokens)\n",
    "        df_row = pd.DataFrame([tag_count], columns=taglist)\n",
    "        pos_dataset = pd.concat([pos_dataset, df_row], ignore_index=True, sort=True)\n",
    "    return pos_dataset.fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking if the labels in the new dataset are correct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "True     714\n",
       "False    553\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Selvaram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Selvaram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')  # Optional, if using word_tokenize elsewhere\n",
    "\n",
    "# Tokenizer function\n",
    "def create_tokens(sentence):\n",
    "    return regexp_tokenize(sentence, pattern=r'\\w+')\n",
    "\n",
    "# POS tag dictionary builder\n",
    "def create_tag_dict(tokens):\n",
    "    tag_tuple = nltk.pos_tag(tokens)\n",
    "    tags = [tag for _, tag in tag_tuple]\n",
    "    tag_set = list(set(tags))\n",
    "    tag_count = {tag: tags.count(tag) for tag in tag_set}\n",
    "    return tag_set, tag_count\n",
    "\n",
    "# POS tagging for entire dataset\n",
    "def create_pos_tag(dataset, taglist):\n",
    "    pos_dataset = pd.DataFrame(columns=taglist)\n",
    "    for _, row in dataset.iterrows():\n",
    "        tokens = create_tokens(row['news'])\n",
    "        tag_list, tag_count = create_tag_dict(tokens)\n",
    "        df_row = pd.DataFrame([tag_count], columns=taglist)\n",
    "        pos_dataset = pd.concat([pos_dataset, df_row], ignore_index=True, sort=True)\n",
    "    pos_dataset = pos_dataset.fillna(0)\n",
    "    pos_dataset['label'] = dataset['label'].values  # Add label column back\n",
    "    return pos_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "True     5752\n",
       "False    4488\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Selvaram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Selvaram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')  # Optional, if using word_tokenize elsewhere\n",
    "\n",
    "# Tokenizer function\n",
    "def create_tokens(sentence):\n",
    "    return regexp_tokenize(sentence, pattern=r'\\w+')\n",
    "\n",
    "# POS tag dictionary builder\n",
    "def create_tag_dict(tokens):\n",
    "    tag_tuple = nltk.pos_tag(tokens)\n",
    "    tags = [tag for _, tag in tag_tuple]\n",
    "    tag_set = list(set(tags))\n",
    "    tag_count = {tag: tags.count(tag) for tag in tag_set}\n",
    "    return tag_set, tag_count\n",
    "\n",
    "# POS tagging for entire dataset\n",
    "def create_pos_tag(dataset, taglist):\n",
    "    pos_dataset = pd.DataFrame(columns=taglist)\n",
    "    for _, row in dataset.iterrows():\n",
    "        tokens = create_tokens(row['news'])\n",
    "        tag_list, tag_count = create_tag_dict(tokens)\n",
    "        df_row = pd.DataFrame([tag_count], columns=taglist)\n",
    "        pos_dataset = pd.concat([pos_dataset, df_row], ignore_index=True, sort=True)\n",
    "    pos_dataset = pos_dataset.fillna(0)\n",
    "    pos_dataset['label'] = dataset['label'].values  # Add label column back\n",
    "    return pos_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Selvaram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Selvaram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')  # Optional, if using word_tokenize elsewhere\n",
    "\n",
    "# Tokenizer function\n",
    "def create_tokens(sentence):\n",
    "    return regexp_tokenize(sentence, pattern=r'\\w+')\n",
    "\n",
    "# POS tag dictionary builder\n",
    "def create_tag_dict(tokens):\n",
    "    tag_tuple = nltk.pos_tag(tokens)\n",
    "    tags = [tag for _, tag in tag_tuple]\n",
    "    tag_set = list(set(tags))\n",
    "    tag_count = {tag: tags.count(tag) for tag in tag_set}\n",
    "    return tag_set, tag_count\n",
    "\n",
    "# POS tagging for entire dataset\n",
    "def create_pos_tag(dataset, taglist):\n",
    "    pos_dataset = pd.DataFrame(columns=taglist)\n",
    "    for _, row in dataset.iterrows():\n",
    "        tokens = create_tokens(row['news'])\n",
    "        tag_list, tag_count = create_tag_dict(tokens)\n",
    "        df_row = pd.DataFrame([tag_count], columns=taglist)\n",
    "        pos_dataset = pd.concat([pos_dataset, df_row], ignore_index=True, sort=True)\n",
    "    pos_dataset = pos_dataset.fillna(0)\n",
    "    pos_dataset['label'] = dataset['label'].values  # Add label column back\n",
    "    return pos_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "True     668\n",
       "False    616\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Selvaram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Selvaram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')  # Optional, if using word_tokenize elsewhere\n",
    "\n",
    "# Tokenizer function\n",
    "def create_tokens(sentence):\n",
    "    return regexp_tokenize(sentence, pattern=r'\\w+')\n",
    "\n",
    "# POS tag dictionary builder\n",
    "def create_tag_dict(tokens):\n",
    "    tag_tuple = nltk.pos_tag(tokens)\n",
    "    tags = [tag for _, tag in tag_tuple]\n",
    "    tag_set = list(set(tags))\n",
    "    tag_count = {tag: tags.count(tag) for tag in tag_set}\n",
    "    return tag_set, tag_count\n",
    "\n",
    "# POS tagging for entire dataset\n",
    "def create_pos_tag(dataset, taglist):\n",
    "    pos_dataset = pd.DataFrame(columns=taglist)\n",
    "    for _, row in dataset.iterrows():\n",
    "        tokens = create_tokens(row['news'])\n",
    "        tag_list, tag_count = create_tag_dict(tokens)\n",
    "        df_row = pd.DataFrame([tag_count], columns=taglist)\n",
    "        pos_dataset = pd.concat([pos_dataset, df_row], ignore_index=True, sort=True)\n",
    "    pos_dataset = pos_dataset.fillna(0)\n",
    "    pos_dataset['label'] = dataset['label'].values  # Add label column back\n",
    "    return pos_dataset\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Selvaram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Selvaram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')  # Optional, if using word_tokenize elsewhere\n",
    "\n",
    "# Tokenizer function\n",
    "def create_tokens(sentence):\n",
    "    return regexp_tokenize(sentence, pattern=r'\\w+')\n",
    "\n",
    "# POS tag dictionary builder\n",
    "def create_tag_dict(tokens):\n",
    "    tag_tuple = nltk.pos_tag(tokens)\n",
    "    tags = [tag for _, tag in tag_tuple]\n",
    "    tag_set = list(set(tags))\n",
    "    tag_count = {tag: tags.count(tag) for tag in tag_set}\n",
    "    return tag_set, tag_count\n",
    "\n",
    "# POS tagging for entire dataset\n",
    "def create_pos_tag(dataset, taglist):\n",
    "    pos_dataset = pd.DataFrame(columns=taglist)\n",
    "    for _, row in dataset.iterrows():\n",
    "        tokens = create_tokens(row['news'])\n",
    "        tag_list, tag_count = create_tag_dict(tokens)\n",
    "        df_row = pd.DataFrame([tag_count], columns=taglist)\n",
    "        pos_dataset = pd.concat([pos_dataset, df_row], ignore_index=True, sort=True)\n",
    "    pos_dataset = pos_dataset.fillna(0)\n",
    "    pos_dataset['label'] = dataset['label'].values  # Add label column back\n",
    "    return pos_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tag_dict(tokens):\n",
    "    tag_tuple = nltk.pos_tag(tokens)\n",
    "    tags = [tag for _, tag in tag_tuple]\n",
    "    tag_set = list(set(tags))\n",
    "    tag_count = {tag: tags.count(tag) for tag in tag_set}\n",
    "    return tag_set, tag_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Selvaram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Selvaram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')  # Optional, if using word_tokenize elsewhere\n",
    "\n",
    "# Tokenizer function\n",
    "def create_tokens(sentence):\n",
    "    return regexp_tokenize(sentence, pattern=r'\\w+')\n",
    "\n",
    "# POS tag dictionary builder using built-in tagger\n",
    "def create_tag_dict(tokens):\n",
    "    tag_tuple = nltk.pos_tag(tokens)\n",
    "    tags = [tag for _, tag in tag_tuple]\n",
    "    tag_set = list(set(tags))\n",
    "    tag_count = {tag: tags.count(tag) for tag in tag_set}\n",
    "    return tag_set, tag_count\n",
    "\n",
    "# POS tagging for entire dataset\n",
    "def create_pos_tag(dataset, taglist):\n",
    "    pos_dataset = pd.DataFrame(columns=taglist)\n",
    "    for _, row in dataset.iterrows():\n",
    "        tokens = create_tokens(row['news'])\n",
    "        tag_list, tag_count = create_tag_dict(tokens)\n",
    "        df_row = pd.DataFrame([tag_count], columns=taglist)\n",
    "        pos_dataset = pd.concat([pos_dataset, df_row], ignore_index=True, sort=True)\n",
    "    pos_dataset = pos_dataset.fillna(0)\n",
    "    pos_dataset['label'] = dataset['label'].values  # Add label column back\n",
    "    return pos_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Selvaram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Selvaram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('averaged_perceptron_tagger_eng')  # Required for newer NLTK versions\n",
    "nltk.download('punkt')  # Optional, if using word_tokenize elsewhere\n",
    "\n",
    "# Tokenizer function\n",
    "def create_tokens(sentence):\n",
    "    return regexp_tokenize(sentence, pattern=r'\\w+')\n",
    "\n",
    "# POS tag dictionary builder using built-in tagger\n",
    "def create_tag_dict(tokens):\n",
    "    tag_tuple = nltk.pos_tag(tokens, lang='eng')  # Explicitly use 'eng' to match downloaded model\n",
    "    tags = [tag for _, tag in tag_tuple]\n",
    "    tag_set = list(set(tags))\n",
    "    tag_count = {tag: tags.count(tag) for tag in tag_set}\n",
    "    return tag_set, tag_count\n",
    "\n",
    "# POS tagging for entire dataset\n",
    "def create_pos_tag(dataset, taglist):\n",
    "    pos_dataset = pd.DataFrame(columns=taglist)\n",
    "    for _, row in dataset.iterrows():\n",
    "        tokens = create_tokens(row['news'])\n",
    "        tag_list, tag_count = create_tag_dict(tokens)\n",
    "        df_row = pd.DataFrame([tag_count], columns=taglist)\n",
    "        pos_dataset = pd.concat([pos_dataset, df_row], ignore_index=True, sort=True)\n",
    "    pos_dataset = pos_dataset.fillna(0)\n",
    "    pos_dataset['label'] = dataset['label'].values  # Add label column back\n",
    "    return pos_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the POS tag datasets in the disk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Selvaram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Selvaram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Selvaram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Selvaram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('averaged_perceptron_tagger_eng')  # Required for newer NLTK versions\n",
    "nltk.download('punkt')  # Optional, if using word_tokenize elsewhere\n",
    "\n",
    "# Tokenizer function\n",
    "def create_tokens(sentence):\n",
    "    return regexp_tokenize(sentence, pattern=r'\\w+')\n",
    "\n",
    "# POS tag dictionary builder\n",
    "def create_tag_dict(tokens):\n",
    "    tag_tuple = nltk.pos_tag(tokens, lang='eng')  # Explicitly use 'eng' to match downloaded model\n",
    "    tags = [tag for _, tag in tag_tuple]\n",
    "    tag_set = list(set(tags))\n",
    "    tag_count = {tag: tags.count(tag) for tag in tag_set}\n",
    "    return tag_set, tag_count\n",
    "\n",
    "# POS tagging for entire dataset\n",
    "def create_pos_tag(dataset, taglist):\n",
    "    pos_dataset = pd.DataFrame(columns=taglist)\n",
    "    for _, row in dataset.iterrows():\n",
    "        tokens = create_tokens(row['news'])\n",
    "        tag_list, tag_count = create_tag_dict(tokens)\n",
    "        df_row = pd.DataFrame([tag_count], columns=taglist)\n",
    "        pos_dataset = pd.concat([pos_dataset, df_row], ignore_index=True, sort=True)\n",
    "    pos_dataset = pos_dataset.fillna(0)\n",
    "    pos_dataset['label'] = dataset['label'].values\n",
    "    return pos_dataset\n",
    "\n",
    "# CSV saving function\n",
    "def save_to_csv(dataset, file_path, file_name):\n",
    "    complete_file_path_with_name = file_path + file_name\n",
    "    dataset.to_csv(complete_file_path_with_name, index=False)\n",
    "import nltk\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('averaged_perceptron_tagger_eng')  # Required for newer NLTK versions\n",
    "nltk.download('punkt')  # Optional, if using word_tokenize elsewhere\n",
    "\n",
    "# Tokenizer function\n",
    "def create_tokens(sentence):\n",
    "    return regexp_tokenize(sentence, pattern=r'\\w+')\n",
    "\n",
    "# POS tag dictionary builder\n",
    "def create_tag_dict(tokens):\n",
    "    tag_tuple = nltk.pos_tag(tokens, lang='eng')  # Explicitly use 'eng' to match downloaded model\n",
    "    tags = [tag for _, tag in tag_tuple]\n",
    "    tag_set = list(set(tags))\n",
    "    tag_count = {tag: tags.count(tag) for tag in tag_set}\n",
    "    return tag_set, tag_count\n",
    "\n",
    "# POS tagging for entire dataset\n",
    "def create_pos_tag(dataset, taglist):\n",
    "    pos_dataset = pd.DataFrame(columns=taglist)\n",
    "    for _, row in dataset.iterrows():\n",
    "        tokens = create_tokens(row['news'])\n",
    "        tag_list, tag_count = create_tag_dict(tokens)\n",
    "        df_row = pd.DataFrame([tag_count], columns=taglist)\n",
    "        pos_dataset = pd.concat([pos_dataset, df_row], ignore_index=True, sort=True)\n",
    "    pos_dataset = pos_dataset.fillna(0)\n",
    "    pos_dataset['label'] = dataset['label'].values\n",
    "    return pos_dataset\n",
    "\n",
    "# CSV saving function\n",
    "def save_to_csv(dataset, file_path, file_name):\n",
    "    complete_file_path_with_name = file_path + file_name\n",
    "    dataset.to_csv(complete_file_path_with_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
